% === Section I: Introduction ===
\section{Introduction}
\label{sec:intro}

A graph is a set of vertices connected by edges.  Each edge is a pair
of vertices---an element of $V \times V$---and the edge set $E$ is a
binary relation over a single domain.  Graphs are ubiquitous: social
networks, web link structures, molecular bonds, communication
topologies.  Graph \emph{algorithms} extract structure from these
relations: shortest paths, connected components, communities, and---the
focus of this paper---triangles.

Conceptually, a graph's edge set is a special case of a \emph{relation}
in the database sense: a set of tuples over typed attributes.  A
relation can connect entities of different types (e.g., authors to
papers, customers to products), can have arity greater than two
(ternary, quaternary, \ldots), and its attributes carry schema
information that graphs lack.  An edge set $E \subseteq V \times V$ is
the degenerate case: a single binary relation where both columns range
over the same untyped domain.

A \emph{hypergraph} generalizes in a different direction: each
hyperedge connects an arbitrary subset of vertices, rather than exactly
two.  Hypergraphs are more general than ordinary graphs but less
general than relations: a hyperedge is an unordered set of vertices
from a single domain, while a relation is an ordered tuple over
potentially distinct, typed domains.  Crucially, the relational
framework admits \emph{joins}---the ability to combine tuples from
multiple relations by matching on shared attributes---and it is this
join machinery, not the data model per se, that gives relational
algebra its optimization power.

There is a third, equally productive analogy.  A graph on $N$ vertices
can be represented as an $N \times N$ adjacency matrix~$A$, where
$A[i,j] = 1$ if edge $(i,j)$ exists.  For most real-world graphs this
matrix is sparse: it has $M$ nonzero entries out of $N^2$ possible,
with $M \ll N^2$.  The GraphBLAS
project~\cite{kepner2016,davis2019} builds on this observation,
providing a standard API that expresses graph algorithms as sparse
linear algebra operations over user-defined semirings.  Breadth-first
search becomes sparse matrix--vector multiplication (SpMV) over the
Boolean semiring; single-source shortest paths becomes SpMV over the
$(\min, +)$ semiring; connected components, PageRank, and many other
algorithms have natural semiring
formulations~\cite{kepner2016}.  This framework is powerful and
productive: the programmer states \emph{what} to compute using matrix
and vector operations, and the runtime chooses \emph{how}---which
sparse data structures, which parallelization strategy, which
scheduling.  GraphBLAS is, in short, a declarative language for graph
computation grounded in the algebra of sparse matrices.

The connection between graph algorithms and relational algebra is not
new: systems such as EmptyHeaded~\cite{aberger2017} have demonstrated
that worst-case optimal joins can serve as the execution engine for
graph pattern queries, achieving substantial speedups over
conventional graph-processing frameworks.  In this paper, we make the
relationship precise and show that it is strictly more powerful than
the linear algebra approach for graph computations that exhibit
\emph{cyclic} structure.  The
distinction is fundamental: sparse linear algebra decomposes
computation into pairwise matrix operations (each combining two rank-2
objects), while relational algebra admits \emph{multiway} operations
that process three or more relations simultaneously.  For acyclic
patterns---paths, trees, star queries---the two frameworks are
equivalent.  For cyclic patterns---triangles, 4-cycles,
cliques---the relational framework produces fused kernels that no
sequence of pairwise matrix operations can express.

Consider triangle counting, the simplest cyclic graph query and the
focus of this paper.  In GraphBLAS, it is expressed as masked sparse
matrix--matrix multiplication: $C\langle L \rangle = L \cdot L$,
where $L$ is the lower-triangular adjacency matrix and the mask
retains only entries corresponding to actual
edges~\cite{aznaveh2020,wolf2017}.  Other cyclic queries follow the
same pattern: 4-cycle counting requires computing $A^2$ (all length-2
paths), and $k$-clique counting decomposes into chains of matrix
multiplications.  In each case, the computation reduces to a sequence
of pairwise matrix products.

The problem is intermediates.  Each pairwise product can produce a
result far larger than the input or the output.  A star graph---a
single hub connected to $\sqrt{M}$ leaves---has $M$ edges and zero
triangles, yet the SpMM $L \cdot L$ enumerates all
$\binom{\sqrt{M}}{2} = \Oh(M)$ length-2 paths through the hub, every
one ultimately discarded by the mask.  On graphs with many
high-degree vertices, the intermediate can reach $\Oh(M^2)$ entries,
even though the triangle count is at most $\Oh(M^{3/2})$ by the AGM
bound~\cite{agm2013}.  For 4-cycles and other cyclic patterns the
situation is worse: no mask exists, and the full intermediate must be
materialized (Section~\ref{sec:beyond}).

For the special case of triangle counting, the GraphBLAS community has
developed a workaround: SuiteSparse implements a \emph{masked
dot-product SpGEMM} that, for each edge $(i,j)$ in the mask~$L$,
computes the inner product of row~$i$ and column~$j$ directly---effectively
intersecting $N(i) \cap N(j)$, the same work that the hand-written
algorithm performs~\cite{davis2019}.  This avoids materializing the
large intermediate and, for triangles, recovers optimal performance.
But the technique is specific to triangles: the mask is the same edge
set being multiplied, the output is indexed by edges (sparse), and a
single inner product per edge suffices to fuse the three-way check.
For other cyclic patterns---4-cycles, diamonds, bow-ties,
$k$-cycles---at least one of these properties fails, no analogous
mask exists, and the pairwise intermediate cannot be avoided
(Section~\ref{sec:beyond}).

The relational algebra framework, by contrast, eliminates these
intermediates in full generality.  The triangle query, expressed as the conjunctive join
$R(x,y) \wedge R(y,z) \wedge R(x,z)$, can be evaluated by
\emph{worst-case optimal join} (WCOJ)
algorithms~\cite{nprr2018,veldhuizen2014} that process all three
relations simultaneously---binding variables one at a time and
intersecting candidate sets at each level, with no intermediate
materialized.  For the star graph, the WCOJ algorithm examines each
edge, finds the neighbor-list intersection empty, and moves
on---$\Oh(M)$ total work for zero triangles.  In general, WCOJ
achieves $\Oh(M^{3/2})$ time, matching the AGM bound and equaling the
complexity of hand-written CSR triangle
counting~\cite{shun2015,green2014}.

Crucially, this improvement is not a matter of lazy evaluation.
Deferring the SpMM---computing entries of $L \cdot L$ on
demand---does not change the structure of the computation: it still
decomposes a three-way query into a pairwise product followed by a
filter, and every surviving entry must still be computed individually.
The complexity difference comes from \emph{eliminating the
intermediate}: rather than forming all length-2 paths and then
checking which ones close a triangle, the WCOJ approach checks the
closing-edge constraint \emph{during} path enumeration, pruning
immediately when the intersection is empty.  This is a structural
difference in the evaluation plan---a different algorithm, not a
different implementation of the same algorithm
(Section~\ref{sec:lazy}).

\paragraph{The gap.}
Meanwhile, the dominant HPC implementation is hand-written: store the
graph in CSR, and for each edge $(x,y)$ with $y > x$, intersect the
sorted neighbor lists of~$x$ and~$y$~\cite{shun2015,green2014}.  This
code is efficient but developed ad~hoc, without connection to either
declarative framework.

\paragraph{This paper.}
We establish the following correspondences between sparse linear
algebra, relational algebra, and hand-written CSR code:
\begin{enumerate}
\item \textbf{Code equivalence via fusion.}  When the WCOJ algorithm
  (Leapfrog Triejoin~\cite{veldhuizen2014}) is evaluated over CSR
  arrays, the generated code is \emph{identical} to hand-written CSR
  triangle counting---the same galloping-intersection inner loop.
  The relational framework \emph{fuses} the three-relation join into a
  single pass; the result is the code that HPC programmers already write
  by hand.

\item \textbf{SpMM as a special case.}  The two-relation join
  \emph{is} SpMM.  Triangle counting---the three-relation
  case---is where the fused approach strictly dominates: instead of a
  separate multiply and filter, it intersects all three constraints in a
  single nested loop.

\item \textbf{Structural incompleteness of linear algebra.}
  Drawing on the SPORES result of Wang et
  al.~\cite{wang2020spores}, we show that this gap is not an
  implementation artifact but an algebraic one: relational algebra
  rewrite rules are \emph{complete} for optimizing linear algebra
  expressions, while the converse does not hold.  No amount of lazy
  evaluation, expression rewriting, or masking strategy within the
  pairwise matrix framework can recover the fused kernel.

\item \textbf{Beyond triangles.}  The masked SpGEMM technique that
  partially closes the gap for triangles does not generalize.  For
  4-cycles, diamonds, bow-ties, and $k$-cycles, no natural mask exists,
  and the pairwise approach is stuck at $\Oh(M^2)$ while WCOJ achieves
  $\Oh(M^{3/2})$ or better.  The dividing line is the \emph{cyclicity}
  of the query hypergraph: any graph pattern containing a cycle requires
  multiway intersection for optimal evaluation.

\item \textbf{Measurable advantage on skewed graphs.}  On graphs
  with high-degree hubs, the pairwise approach produces a large
  intermediate while the fused approach does not.  We demonstrate this
  blowup experimentally.
\end{enumerate}

\noindent
Taken together, these results show that relational algebra is a more
natural and more powerful mechanism for expressing graph algorithms
than linear algebra.  Graph edges \emph{are} relations, and the
relational framework---with its multiway join machinery---subsumes
the pairwise matrix operations of sparse linear algebra while also
generating kernels that linear algebra cannot express.  The
hand-written CSR code that HPC programmers have long used is, in
fact, a worst-case optimal relational join.

\paragraph{Paper organization.}
Section~\ref{sec:background} reviews CSR format, GraphBLAS triangle
counting, the AGM bound, and how CSR supports variable-at-a-time
evaluation.
Section~\ref{sec:spmm} shows that the two-relation join recovers SpMM.
Section~\ref{sec:triangle} develops the three-way triangle join,
derives the WCOJ evaluation plan, and presents the intersection kernel.
Section~\ref{sec:equivalence} presents the central code-equivalence
result.
Section~\ref{sec:production} describes a production realization.
Section~\ref{sec:lazy} explains why lazy evaluation of the pairwise
approach cannot close the gap, drawing on the SPORES completeness
result.
Section~\ref{sec:beyond} extends the analysis to 4-cycles, diamonds,
and other cyclic patterns.
Section~\ref{sec:experiments} gives experimental results.
Sections~\ref{sec:related}--\ref{sec:conclusion} discuss related work
and conclude.
